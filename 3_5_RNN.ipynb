{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data and Recurring Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"src//lib.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll summarise two related and important topics in Neural Networks:\n",
    "* Text processing and how to feed text data into Neural Networks\n",
    "* The Neural Networks models that have been developed to handle data having a sequence structure (i.e. text or timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization\n",
    "In the previous notebooks we have seen that neural networks model must be fed with numeric tensors, the *text vectorization* is a process that transforms text data into quantitative data.\n",
    "\n",
    "There are different way to *vectorize* text data, as example:\n",
    "* Segment text into **words** and associate each word to a numerical vector.\n",
    "* Segment text into **characters** and transform each character into a numerical vector.\n",
    "* Extract **N-Grams** of words/characters and transform each N-Gram into a vector. A **N-gram** is a sequence of at most **N** consecutive words/characters in a text data: consider text *\"UniCredit is a pan European Winner\"*, it contains the following **3 grams**: *Unicredit, is, a, pan, European, Winner, Unicredit is, is a, a pan, pan European, European Winner, Unicredit is a, is a pan, a pan European, pan European Winner*.\n",
    "\n",
    "In the following we'll focus our attention on the first *vectorization* strategy: **words** vectorization.\n",
    "\n",
    "The **first step** in this process is to segment text data into words: *Tokenization*. Let's see how to tokenize text data in **KERAS**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'unicredit'</li>\n",
       "\t<li>'is'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'pan'</li>\n",
       "\t<li>'european'</li>\n",
       "\t<li>'winner'</li>\n",
       "\t<li>'cib'</li>\n",
       "\t<li>'fully'</li>\n",
       "\t<li>'plugged'</li>\n",
       "\t<li>'into'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'unicredit'\n",
       "\\item 'is'\n",
       "\\item 'a'\n",
       "\\item 'pan'\n",
       "\\item 'european'\n",
       "\\item 'winner'\n",
       "\\item 'cib'\n",
       "\\item 'fully'\n",
       "\\item 'plugged'\n",
       "\\item 'into'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'unicredit'\n",
       "2. 'is'\n",
       "3. 'a'\n",
       "4. 'pan'\n",
       "5. 'european'\n",
       "6. 'winner'\n",
       "7. 'cib'\n",
       "8. 'fully'\n",
       "9. 'plugged'\n",
       "10. 'into'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"unicredit\" \"is\"        \"a\"         \"pan\"       \"european\"  \"winner\"   \n",
       " [7] \"cib\"       \"fully\"     \"plugged\"   \"into\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples <- c(\"UniCredit is a pan European Winner\", \"CIB is fully plugged into UniCredit\")\n",
    "tokenizer <- text_tokenizer() %>%\n",
    " fit_text_tokenizer(samples)\n",
    "\n",
    "tokenizer$word_counts %>% names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer found 10 different *words* in the input sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot encoding\n",
    "Now we have to associate to these words (*tokens*) a numeric vector, the more simple way is called **One-Hot encoding**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoding strategy first requires to associates a unique integer index id $H_w$ to each word $w$. In **KERAS** this is done directly by the *tokenizer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$unicredit</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$is</dt>\n",
       "\t\t<dd>2</dd>\n",
       "\t<dt>$a</dt>\n",
       "\t\t<dd>3</dd>\n",
       "\t<dt>$pan</dt>\n",
       "\t\t<dd>4</dd>\n",
       "\t<dt>$european</dt>\n",
       "\t\t<dd>5</dd>\n",
       "\t<dt>$winner</dt>\n",
       "\t\t<dd>6</dd>\n",
       "\t<dt>$cib</dt>\n",
       "\t\t<dd>7</dd>\n",
       "\t<dt>$fully</dt>\n",
       "\t\t<dd>8</dd>\n",
       "\t<dt>$plugged</dt>\n",
       "\t\t<dd>9</dd>\n",
       "\t<dt>$into</dt>\n",
       "\t\t<dd>10</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$unicredit] 1\n",
       "\\item[\\$is] 2\n",
       "\\item[\\$a] 3\n",
       "\\item[\\$pan] 4\n",
       "\\item[\\$european] 5\n",
       "\\item[\\$winner] 6\n",
       "\\item[\\$cib] 7\n",
       "\\item[\\$fully] 8\n",
       "\\item[\\$plugged] 9\n",
       "\\item[\\$into] 10\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$unicredit\n",
       ":   1\n",
       "$is\n",
       ":   2\n",
       "$a\n",
       ":   3\n",
       "$pan\n",
       ":   4\n",
       "$european\n",
       ":   5\n",
       "$winner\n",
       ":   6\n",
       "$cib\n",
       ":   7\n",
       "$fully\n",
       ":   8\n",
       "$plugged\n",
       ":   9\n",
       "$into\n",
       ":   10\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$unicredit\n",
       "[1] 1\n",
       "\n",
       "$is\n",
       "[1] 2\n",
       "\n",
       "$a\n",
       "[1] 3\n",
       "\n",
       "$pan\n",
       "[1] 4\n",
       "\n",
       "$european\n",
       "[1] 5\n",
       "\n",
       "$winner\n",
       "[1] 6\n",
       "\n",
       "$cib\n",
       "[1] 7\n",
       "\n",
       "$fully\n",
       "[1] 8\n",
       "\n",
       "$plugged\n",
       "[1] 9\n",
       "\n",
       "$into\n",
       "[1] 10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer$word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to transform our sample sentences into list of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'list'"
      ],
      "text/latex": [
       "'list'"
      ],
      "text/markdown": [
       "'list'"
      ],
      "text/plain": [
       "[1] \"list\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences <- texts_to_sequences(tokenizer, samples)\n",
    "class(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence associated with *UniCredit is a pan European Winner* is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>5</li>\n",
       "\t<li>6</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 6\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 2\n",
       "3. 3\n",
       "4. 4\n",
       "5. 5\n",
       "6. 6\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 2 3 4 5 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences[[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence associated with *CIB is fully plugged into UniCredit* is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>7</li>\n",
       "\t<li>2</li>\n",
       "\t<li>8</li>\n",
       "\t<li>9</li>\n",
       "\t<li>10</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 7\n",
       "\\item 2\n",
       "\\item 8\n",
       "\\item 9\n",
       "\\item 10\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 7\n",
       "2. 2\n",
       "3. 8\n",
       "4. 9\n",
       "5. 10\n",
       "6. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  7  2  8  9 10  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences[[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the two words *UniCredit* (coded by **1**) and *is* (coded by **2**) are the only ones contained in both sentences, and as a consequence their codes are repeated in both sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally** we need to transform each number into a binary vector $N$-dimensional binary vector (where $N$ is the number of unique words in all the sentences) in which the only not-zero component is the one corresponding to the given number. This final step is currently done by **KERAS** in a more smart way (as we'll see soon), btw here is a simple base *R* code to do compute this final step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- array(0, dim = c(\n",
    "  length(sequences), # number of sentences\n",
    "  max(sapply(sequences, length)), # maximum number of words in a sentence\n",
    "  max(sapply(sequences, max)) # number of unique words in all sentences\n",
    "))\n",
    "\n",
    "for (i in 1:length(sequences)) {\n",
    "  sequence <- sequences[[i]]\n",
    "  for(h in 1:length(sequence)) {\n",
    "    results[i, h, sequence[h]] <- 1\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sentences is **one-hot encoded** by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llllllllll}\n",
       "\t 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "| 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "| 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "| 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "| 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | \n",
       "| 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n",
       "[1,] 1    0    0    0    0    0    0    0    0    0    \n",
       "[2,] 0    1    0    0    0    0    0    0    0    0    \n",
       "[3,] 0    0    1    0    0    0    0    0    0    0    \n",
       "[4,] 0    0    0    1    0    0    0    0    0    0    \n",
       "[5,] 0    0    0    0    1    0    0    0    0    0    \n",
       "[6,] 0    0    0    0    0    1    0    0    0    0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results[1,,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second one by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llllllllll}\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\\n",
       "\t 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
       "\t 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | \n",
       "| 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | \n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | \n",
       "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | \n",
       "| 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n",
       "[1,] 0    0    0    0    0    0    1    0    0    0    \n",
       "[2,] 0    1    0    0    0    0    0    0    0    0    \n",
       "[3,] 0    0    0    0    0    0    0    1    0    0    \n",
       "[4,] 0    0    0    0    0    0    0    0    1    0    \n",
       "[5,] 0    0    0    0    0    0    0    0    0    1    \n",
       "[6,] 1    0    0    0    0    0    0    0    0    0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results[2,,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is very simple and immediate but have a great disadvantage: the dimension of the vector it produces scales with $N$, the size of the considered vocabulary. Is not **BIG DATA** working with a vocabulary with thousands of words: in these cases we need a more dense representation.\n",
    "\n",
    "The idea behind **words embedding** is to reduce the dimensionality of the binary sparse vectors produced by the **one-hot encoding** in to more compact space of **numeric vectors**:\n",
    "\n",
    "<img src=\"fig/words_embedding.PNG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mapping should preserve some semantic structure of the vocabulary it represents, in some way the geometric relationships between word vectors should reflect the semantic relationships between these words. \n",
    "\n",
    "Otherwise, if we'd use a random mapping the **neural network** couldn't be very effective since two words with a similar meanings like *handsome* and *attractive* could be mapped into very different vectors:  word embeddings are meant to\n",
    "map human language into a geometric space (similar words should be \"near\"):\n",
    "\n",
    "<img src=\"fig/words_plot.PNG\" width=\"400\">\n",
    "\n",
    "But how to find this type of mapping? Here we consider two ways:\n",
    "* Use one of the words embedding mapping already computed by other people, like the [Word2Vec](http://code.google.com/archive/p/word2vec) or the [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "* Learn the right embedding for your task while learning the rest of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words embedding on IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll see only how to find the right embedding while solving a text oriented task. But first we need a task!\n",
    "\n",
    "Here we consider the [Internet Movie DataBase (IMDB)](https://www.imdb.com/) sentiment analys dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features <- 10000\n",
    "\n",
    "imdb <- dataset_imdb(num_words = max_features)\n",
    "x_train <- imdb$train$x\n",
    "y_train <- imdb$train$y\n",
    "x_test <- imdb$test$x\n",
    "y_test <- imdb$test$y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a sample dataset provided by **KERAS** and as a consequence has been already preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'list'"
      ],
      "text/latex": [
       "'list'"
      ],
      "text/markdown": [
       "'list'"
      ],
      "text/plain": [
       "[1] \"list\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "25000"
      ],
      "text/latex": [
       "25000"
      ],
      "text/markdown": [
       "25000"
      ],
      "text/plain": [
       "[1] 25000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>14</li>\n",
       "\t<li>22</li>\n",
       "\t<li>16</li>\n",
       "\t<li>43</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 14\n",
       "\\item 22\n",
       "\\item 16\n",
       "\\item 43\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 14\n",
       "3. 22\n",
       "4. 16\n",
       "5. 43\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  1 14 22 16 43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train %>% class\n",
    "x_train %>% length\n",
    "x_train[[1]] %>% head(n = 5)\n",
    "y_train[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input features are the indexes associated with the words in a movie review, in the target binary class we have an indicator telling if the review is positive (1) or negative (0). Note that parameter *num_words* set to 10000 limit the considered dictionary to the 10000 words with the highest frequency. Using function **dataset_imdb_word_index** we can retrieve the original content of the review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'&lt;START&gt; big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i\\'ve seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it\\'s just so damn terribly written the clothes are sickening and funny in equal &lt;UNK&gt; the hair is big lots of boobs &lt;UNK&gt; men wear those cut &lt;UNK&gt; shirts that show off their &lt;UNK&gt; sickening that men actually wore them and the music is just &lt;UNK&gt; trash that plays over and over again in almost every scene there is trashy music boobs and &lt;UNK&gt; taking away bodies and the gym still doesn\\'t close for &lt;UNK&gt; all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80\\'s and have a good old laugh at how bad everything was back then'"
      ],
      "text/latex": [
       "'<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i\\textbackslash{}'ve seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it\\textbackslash{}'s just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn\\textbackslash{}'t close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80\\textbackslash{}'s and have a good old laugh at how bad everything was back then'"
      ],
      "text/markdown": [
       "'&lt;START&gt; big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i\\'ve seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it\\'s just so damn terribly written the clothes are sickening and funny in equal &lt;UNK&gt; the hair is big lots of boobs &lt;UNK&gt; men wear those cut &lt;UNK&gt; shirts that show off their &lt;UNK&gt; sickening that men actually wore them and the music is just &lt;UNK&gt; trash that plays over and over again in almost every scene there is trashy music boobs and &lt;UNK&gt; taking away bodies and the gym still doesn\\'t close for &lt;UNK&gt; all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80\\'s and have a good old laugh at how bad everything was back then'"
      ],
      "text/plain": [
       "[1] \"<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn't close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_step <- 3  # word index offset\n",
    "data <- dataset_imdb_word_index()\n",
    "mapping_df <- data_frame(\n",
    "  word = names(data),\n",
    "  index = data %>% sapply(function(x)x[[1]]) + index_step\n",
    ") %>% rbind(\n",
    "  data_frame(\n",
    "    word = c(\"<PAD>\", \"<START>\",\"<UNK>\"),\n",
    "    index = 0:2\n",
    "  )\n",
    ")\n",
    "\n",
    "x_train[[2]] %>% sapply(function(x) mapping_df %>% filter(index == x) %>% pull(word)) %>% paste(collapse = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text is quite a nonsense since we are removed all the words that aren't the most frequent ones. Another way to simplify the input data is to limit (or pad) each review to the first *K* words with **KERAS** function *pad_sequences* (here $K = 20$), at the end we have an input matrix having a row for each review and *K* columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'matrix'"
      ],
      "text/latex": [
       "'matrix'"
      ],
      "text/markdown": [
       "'matrix'"
      ],
      "text/plain": [
       "[1] \"matrix\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>25000</li>\n",
       "\t<li>20</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 25000\n",
       "\\item 20\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 25000\n",
       "2. 20\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 25000    20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'on the disaster that was the 80\\'s and have a good old laugh at how bad everything was back then'"
      ],
      "text/latex": [
       "'on the disaster that was the 80\\textbackslash{}'s and have a good old laugh at how bad everything was back then'"
      ],
      "text/markdown": [
       "'on the disaster that was the 80\\'s and have a good old laugh at how bad everything was back then'"
      ],
      "text/plain": [
       "[1] \"on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxlen <- 20\n",
    "x_train <- pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test <- pad_sequences(x_test, maxlen = maxlen)\n",
    "class(x_train)\n",
    "dim(x_train)\n",
    "x_train[2,] %>% sapply(function(x) mapping_df %>% filter(index == x) %>% pull(word)) %>% paste(collapse = \" \")                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to learn our embedding while training a neural network on the imdb dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "embedding_1 (Embedding)             (None, 20, 8)                   80000       \n",
      "________________________________________________________________________________\n",
      "flatten_1 (Flatten)                 (None, 160)                     0           \n",
      "________________________________________________________________________________\n",
      "dense_1 (Dense)                     (None, 1)                       161         \n",
      "================================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "layer_embedding(input_dim = 10000, output_dim = 8, input_length = maxlen) %>%\n",
    "layer_flatten() %>%\n",
    "layer_dense(units = 1, activation = \"sigmoid\")\n",
    "model %>% compile(\n",
    "    optimizer = \"rmsprop\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = c(\"acc\")\n",
    ")\n",
    "summary(model)\n",
    "history <- model %>% fit(\n",
    "    x_train, y_train,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAP1BMVEUAAAAAv8QzMzNNTU1o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\nrkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
