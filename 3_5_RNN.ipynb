{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data and Recurring Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"src//lib.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll summarise two related topics in Neural Networks:\n",
    "* Text processing and how to feed text data into Neural Networks\n",
    "* The Neural Networks models that have been developed to handle data having a sequence structure (i.e. text or timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text vectorization\n",
    "In the previous notebooks we have seen that neural networks model must be fed with numeric tensors, the *text vectorization* is a process that transforms text data into quantitative data.\n",
    "\n",
    "There are different way to *vectorize* text data, as example:\n",
    "* Segment text into **words** and associate each word to a numerical vector.\n",
    "* Segment text into **characters** and transform each character into a numerical vector.\n",
    "* Extract **N-Grams** of words/characters and transform each N-Gram into a vector. A **N-gram** is a sequence of at most **N** consecutive words/characters in a text data: consider text *\"UniCredit is a pan European Winner\"*, it contains the following **3 grams**: *Unicredit, is, a, pan, European, Winner, Unicredit is, is a, a pan, pan European, European Winner, Unicredit is a, is a pan, a pan European, pan European Winner*.\n",
    "\n",
    "In the following we'll focus our attention on the first *vectorization* strategy: **words** vectorization.\n",
    "\n",
    "The **first step** in this process is to segment text data into words, aka *Tokenization*. Let's see how to tokenize text data in using **KERAS**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$pan</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$winner</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$unicredit</dt>\n",
       "\t\t<dd>2</dd>\n",
       "\t<dt>$is</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$a</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$european</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$plugged</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$cib</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$into</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$fully</dt>\n",
       "\t\t<dd>1</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$pan] 1\n",
       "\\item[\\$winner] 1\n",
       "\\item[\\$unicredit] 2\n",
       "\\item[\\$is] 1\n",
       "\\item[\\$a] 1\n",
       "\\item[\\$european] 1\n",
       "\\item[\\$plugged] 1\n",
       "\\item[\\$cib] 1\n",
       "\\item[\\$into] 1\n",
       "\\item[\\$fully] 1\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$pan\n",
       ":   1\n",
       "$winner\n",
       ":   1\n",
       "$unicredit\n",
       ":   2\n",
       "$is\n",
       ":   1\n",
       "$a\n",
       ":   1\n",
       "$european\n",
       ":   1\n",
       "$plugged\n",
       ":   1\n",
       "$cib\n",
       ":   1\n",
       "$into\n",
       ":   1\n",
       "$fully\n",
       ":   1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$pan\n",
       "[1] 1\n",
       "\n",
       "$winner\n",
       "[1] 1\n",
       "\n",
       "$unicredit\n",
       "[1] 2\n",
       "\n",
       "$is\n",
       "[1] 1\n",
       "\n",
       "$a\n",
       "[1] 1\n",
       "\n",
       "$european\n",
       "[1] 1\n",
       "\n",
       "$plugged\n",
       "[1] 1\n",
       "\n",
       "$cib\n",
       "[1] 1\n",
       "\n",
       "$into\n",
       "[1] 1\n",
       "\n",
       "$fully\n",
       "[1] 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples <- c(\"UniCredit is a pan European Winner\", \"CIB fully plugged into UniCredit\")\n",
    "tokenizer <- text_tokenizer() %>%\n",
    " fit_text_tokenizer(samples)\n",
    "\n",
    "tokenizer$word_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **second step** required to associate a numeric vector to each token produced by the first step. The first simple way to associate a numeric vector to each token is called **One-Hot encoding**:\n",
    "* Associate to each word $w$ a unique integer index id $H_w$\n",
    "* Transform the integer index into a binary $N$-dimensional binary vector (where $N$ is the number of unique tokens) in which the only not-zero component is the one corresponding to index $H_w$\n",
    "\n",
    "Both sub steps are implemented into the **Keras** framework. First associate each word to an integer index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>5</li>\n",
       "\t<li>6</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>7</li>\n",
       "\t<li>8</li>\n",
       "\t<li>9</li>\n",
       "\t<li>10</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 6\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 7\n",
       "\\item 8\n",
       "\\item 9\n",
       "\\item 10\n",
       "\\item 1\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 1. 1\n",
       "2. 2\n",
       "3. 3\n",
       "4. 4\n",
       "5. 5\n",
       "6. 6\n",
       "\n",
       "\n",
       "\n",
       "2. 1. 7\n",
       "2. 8\n",
       "3. 9\n",
       "4. 10\n",
       "5. 1\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] 1 2 3 4 5 6\n",
       "\n",
       "[[2]]\n",
       "[1]  7  8  9 10  1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(sequences <- texts_to_sequences(tokenizer, samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
